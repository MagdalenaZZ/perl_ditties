package generic::datalayer;

use strict;
use warnings;
use Data::Dumper;
use Carp('confess');
use IO::Handle;

our $max_length = 10000000;
our $dir = '/raid/local/SQLLDR/';
#our $dir = '/san/TEMP/SQLLDR/';


sub clear_sqlldr {
	print "clearing sqlldr directory...\n";
	`find $dir -type f -delete`;
}


sub run_sqlldr {
	my $user = shift || die 'need user';
	my $password = shift || die 'need password';
	my $table = shift || die 'need table';

	# find all .ctls

	# loop through and run and print them

	`sqlldr $user/$password control=keyword_lookup.ctl`;
}


sub get_max {
	my $schema = shift || die 'error';
	my $table = shift || die 'error';
	my $primary_key = shift || die 'error';
	
	my $sql = 'select max('.$primary_key.') max_id from '.$schema.'.'.$table;
	#print Dumper($sql);	
	my $sth = $::dbh->prepare($sql);
	$sth->execute();
	if (my $row = $sth->fetchrow_hashref) {
		if (!defined($row->{MAX_ID})) {
			return 0;
		}
		else {
			return $row->{MAX_ID} + 1;
		}
	}
}



=head1
	truncates specified table, 
	then runs sqlldr with specified user/schema and default password, 
	uses files generated by insert_record3 method
=cut
sub populate_records3 {
	my $schema = shift || die 'need schema';
	my $table = shift || die 'need table';
	my $truncate = shift;

	$schema = lc($schema);
	$table = lc($table);

	if (!defined($truncate)) {
		$truncate = 1 
	}

	
	my $fields = $::cache->{field_info}->{$schema}->{$table};
	my $filename_extra = '';
	my $filename = $schema.'.'.$table.'_'.$filename_extra;
	my $data_file = "$filename.dat";
	my $control_file = "$filename.ctl";

	create_ctl_file($schema, $table);
	
	if ($truncate) {
		eval {
			$::dbh->prepare("truncate table $schema.$table")->execute();
		};
	}

	cbc::common::chdir($dir);
	my $readsize = $::max_line_lengths->{$schema}->{$table} || 10;
	cbc::common::run_tee3("sqlldr $schema/ore28gon direct=true readsize=$readsize control=$control_file");

	if (defined($::fh->{$data_file}) && $::fh->{$data_file}->opened()) {
		close($::fh->{$data_file});
	}
}



sub create_ctl_file {
	my $schema = shift || die 'need schema';
	my $table = shift || die 'need table';
	$schema = lc($schema);
	$table = lc($table);

	my $filename_extra = '';
	my $filename = $schema.'.'.$table.'_'.$filename_extra;
	my $fields = $::cache->{field_info}->{$schema}->{$table};

	#[$table."_.ctl", $table."_.dat"],
	my $files = [
		["$filename.ctl","$filename.dat"]
	];

	foreach my $fileset (@$files) {
		my ($control_file, $data_file) = @$fileset;
		#print "writing ctl file...";
		open ($::fh->{$control_file}, '>', $dir.$control_file) || die 'file error'.$dir.$control_file;
		my @sqlldr_fields = ();
		foreach my $field (@$fields) {
			my $sqlldr_field;
			if (grep {$field eq $_} ('INSERT_DATE', 'UPDATE_DATE','DATE_ADDED','ENTRY_DATE','REVISED_DATE','DATA_COLLECTION_DATE')) {
				$sqlldr_field = "$field TIMESTAMP 'YYYY-MM-DD HH24:MI:SS'";
			}
			else {
				my $length = 1;
				my $max_field_length = $::max_field_lengths->{$schema}->{$table}->{$field};
				if (defined($max_field_length) && $max_field_length) {
					$length = $max_field_length;
				}
				# having 0 length char causes weird problems with sqlldr, like rejected due to null value when not null, strange

				$sqlldr_field = "$field CHAR($length)";
				#$max_field_length)";
			}
			push @sqlldr_fields, $sqlldr_field;

		}
		print { $::fh->{$control_file} } "LOAD DATA\nINFILE '$dir$data_file' \"str '<endrec>\n'\"\nAPPEND INTO TABLE $table\nFIELDS TERMINATED BY ',' optionally enclosed by '\|-oo-|'\n(".join(',',@sqlldr_fields).')';
		close ($::fh->{$control_file});
	}

	return;
}




# combine .dat files into one big one (lots of separate sqlldr commands takes too long)
sub combine_records3 {
	my $table = shift || die 'need table';
	$table = lc($table.'_');
	cbc::common::chdir($dir);
	my $cmd = "find -maxdepth 1 -type f -name '$table*.dat' | grep -v '$table.dat'";
	my @files = split(/\n/, `$cmd`);

	#print Dumper($cmd, \@files); exit;
	system("cp /dev/null $table.dat");

	foreach my $file (@files) {
		system("cat $file >> $table.dat");
	}
	return;
}

sub initialise_field_info {

	if (!defined($::cache->{field_info})) {
		#print STDERR "getting column list (once only) ....\n";
		my $sql = "select COLUMN_NAME, OWNER, TABLE_NAME from dba_tab_columns where owner not like '%SYS%' and table_name not like 'TEMP%' and table_name not like 'BIN\$%'";
		my $sth = $::dbh->prepare($sql);
		$sth->execute();
		while(my $row = $sth->fetchrow_hashref) {
			#print Dumper($row);
			my $schema = lc($row->{OWNER});
			my $table = lc($row->{TABLE_NAME});
			#next if ($row->{COLUMN_NAME} eq 'ADDED'); # temporary hack to enable default sysdate to work
			if (!defined($::cache->{field_info}->{$schema}->{$table})) {
				$::cache->{field_info}->{$schema}->{$table} = [];
			}
			my $fields = $::cache->{field_info}->{$schema}->{$table};
			push @$fields, $row->{COLUMN_NAME};
		}

		#print Dumper($::cache->{field_info});
	}
	
	return;
}



=head1
	inserts records in sqlldr mode but accepts an extra parameter to allow multiple .dat/.ctl combinations to be used
=cut
sub insert_record3 {
	my $schema = shift || die 'error';
	my $table = shift || die 'error';
	my $filename_extra = shift || '';
	my $record = shift || die 'error';

	$schema = lc($schema);
	$table = lc($table);

	initialise_field_info();
	my $fields = $::cache->{field_info}->{$schema}->{$table};
	my $filename = $schema.'.'.$table.'_'.$filename_extra;
	my $data_file = "$filename.dat";
	my @values = ();

	foreach my $field (@$fields) {
		my $value = $record->{$field};
		if (!defined($value)) {
			$value = '';
		}
		push @values, $value;
		my $new_length = length($value);
		my $existing_length = $::max_field_lengths->{$schema}->{$table}->{$field};
		if (!defined($existing_length) || $new_length > $existing_length) {
			$::max_field_lengths->{$schema}->{$table}->{$field} = $new_length;
		}
	}

	#print Dumper($::max_field_lengths);
	#<STDIN>;


	# create data_file if not already existing
	if (!defined($::fh->{$data_file}) || !$::fh->{$data_file}->opened()) {
		#print 'opening';
		# open filehandle once and store in a global variable
		my $data_file_path = $dir.$data_file;
		open ($::fh->{$data_file}, '>', $data_file_path) || die 'file error';
		#seek ($::fh->{$data_file}, 0);
	}
	#print Dumper($::fh);

	@values = map { '|-oo-|'.$_.'|-oo-|' } @values;
	# bindsize should include the newline character as well
	my $line = join(",", @values)."<endrec>\n";
	my $new_length = length($line);
	my $existing_length = $::max_line_lengths->{$schema}->{$table};
	if (!defined($existing_length) || $new_length > $existing_length) {
		$::max_line_lengths->{$schema}->{$table} = $new_length;
	}

	my $length = length($line);
	if ($length > $max_length) {
		warn 'line exceeds maximum length permitted by sqlldr('.$max_length.') '.$length.' skipping';
		return;
	}

	print { $::fh->{$data_file} } $line;
}



sub purge_sqlldr_data_file {
	my $data_file = shift || die 'no handle';
	close ($::fh->{$data_file});
	open ($::fh->{$data_file}, '>', $dir.$data_file) || die 'file error';
}



sub insert_record2 {
	my $schema = shift || die 'error';
	my $table = shift || die 'error';
	my $record = shift || die 'error';
	my $mode = shift || 'normal';
	
	my @fields = sort keys(%$record);
	my @values = map { $record->{$_} } sort keys(%$record);

	my $control_file_no;
	my $new_file_no;
	my $control_file;
	my $data_file;




	foreach my $file_no (sort {$a <=> $b} (keys(%{$::sqlldr_files->{$table}}))) {
		if ($::sqlldr_files->{$table}->{$file_no} eq join(',',@fields)) {
			$control_file_no = $file_no;
		}
		$new_file_no = $file_no;
	}

	if (defined($new_file_no)) {
		$new_file_no++;
	}
	else {
		$new_file_no = 1;
	}



	if ($mode eq 'sqlldr') {
		if (defined($control_file_no)) {
			$control_file = $table.$control_file_no.'.ctl';
			$data_file = $table.$control_file_no.'.dat';
		}
		else {
			$control_file_no = $new_file_no;
			$::sqlldr_files->{$table}->{$control_file_no} = join(',', @fields);
			$control_file = $table.$control_file_no.'.ctl';
			$data_file = $table.$control_file_no.'.dat';
			
			my $CTL;
			open ($CTL, '>', '/san/TEMP/SQLLDR/'.$control_file);
			@fields = map { "$_ CHAR(10000000)" } @fields;
			print $CTL "LOAD DATA\nINFILE '/san/TEMP/SQLLDR/$data_file' \"str '<endrec>\n'\"\nAPPEND INTO TABLE $table\nFIELDS TERMINATED BY ',' optionally enclosed by '\|-oo-|'\n("
			.join(',',@fields).')';
			close ($CTL);
		}
	}

	if ($mode eq 'normal') {
		my $sql = 'insert into '.$schema.'.'.$table.' ('.join(',',@fields).') values('.join(',', map { '?' } @fields).')';
		#print Dumper($sql, \@values);
		my $sth = $::dbh->prepare($sql);
		$sth->execute(@values);
	}
	elsif($mode eq 'sqlldr') {
		
		# open filehandle once and store in a global variable
		if (!defined($::fh->{$data_file})) {
			open ($::fh->{$data_file}, '>', '/san/TEMP/SQLLDR/'.$data_file) || die 'file error';
			#print Dumper($::fh, 'opening file handle');
		}

		#@values = map { !defined($_) ? '' : $_ } @values;
		@values = map { '|-oo-|'.$_.'|-oo-|' } @values;
		print { $::fh->{$data_file} } join(",", @values)."<endrec>\n";
	}
}








=head1 insert_record 
	THIS METHOD IS TOO COMPLEX, DEPRACTE IN FAVOUR OF ABOVE

	parameter 1 = table_name
	parameter 2 = values ( column_name => value hashref )
	parameter 3 = mode (optional)

	returns value (hashref):
		record_before => record before any insert/update activity
		record => record after any insert/update activity
		action => insert | update
		no_fields_modified => 0

	
	5 modes:
	select_only
	insert_only (default)
	update_overwrite_if_null
	update_overwrite_if_shorter
	update_overwrite_all

	(rename to insert_record)
=cut



sub insert_record {
	my $self = shift;
	my $table = shift;
	my $values = shift || confess '2nd argument must be values_hashref';
	my $mode = shift || 'insert';
	$table = lc($table);
	
	die 'invalid mode for insert_record' if (!grep { $_ eq $mode } 
	(
		'select_only',
		'insert',
		'update_overwrite_if_null',
		'update_overwrite_if_shorter',
		'update_overwrite_all'
	));
	

	my $querys = {
		'select_by_unique_fields' 	=> 'select_'.$table.'_by_unique_fields_new',
		'select_max' 			=> 'select_'.$table.'_max_new',
		'insert'			=> 'insert_'.$table.'_new',
		'update'			=> 'update_'.$table.'_new'	
	};

	foreach (keys(%$querys)) {
		if (!defined($::querys->{$querys->{$_}})) {
			# confess "\n\n$querys->{$_} not defined";
			# commented out to deal with tables that have no primary key or unique fields
		}
	}

	# populate primary key if not already defined
	if (defined($::tables->{$table}->{primary_key})) {
		my $primary_key = $::tables->{$table}->{primary_key};

		if (!defined($values->{$primary_key})) {
			$values->{$primary_key} = __PACKAGE__->fetchrow_hashref($querys->{select_max})->{$primary_key};
			$values->{$primary_key}++;
		}
	}
	
	
	my @fields_updated;
	my $action = 'none';
	my @unique_fields = map { $values->{$_} } @{$::tables->{$table}->{unique_fields}};
	my %debug_unique_fields = map { $_ => $values->{$_} } @{$::tables->{$table}->{unique_fields}};

	#confess "\none or more unique fields not defined\n"
	#	.Dumper(\%debug_unique_fields) if (grep { !defined($_) } @unique_fields);
	
	my $record = __PACKAGE__->fetchrow_hashref($querys->{select_by_unique_fields}, @unique_fields);



=head1
	if record found	do an update if in update mode
=cut

	if ($record) {
		if ($mode =~ /update/) {

			foreach (@{$::tables->{$table}->{fields}}) {
				if (
					($mode eq 'update_overwrite_if_shorter' 
					&& (
						(!defined($record->{$_})) 
						||
						(
						defined($record->{$_}) 
						&& defined($values->{$_})
						&& (length($record->{$_}) < length($values->{$_}))
						)
					)
					)
					||
					($mode eq 'update_overwrite_if_null'
					&& !defined($record->{$_})
					&& defined($values->{$_})
					)
					||
					($mode eq 'update_overwrite_all'
					)	
				) {
					#print "\n$mode $_ ... $record->{$_} -> $values->{$_}";
					push @fields_updated, $_;
				}
				else {
					# retain existing value
					$values->{$_} = $record->{$_};
				}
			}

			if (@fields_updated) {
				my @fields = map { $values->{$_} } @{$::tables->{$table}->{fields}};
				$::querys->{$querys->{update}}->execute(@fields, @unique_fields);
				$record = __PACKAGE__->fetchrow_hashref($querys->{select_by_unique_fields}, @unique_fields);
				$action = 'updated';
			}
		}
	}

=pod
	if no record found and is not select_only do an insert
=cut

	elsif (!$record && $mode ne 'select_only') {
		# translate the values hashref into a values array using the global tables hashref
		my @fields = map { $values->{$_} } @{$::tables->{$table}->{fields}};
		#print Dumper(\@fields); print Dumper($values); exit;
		eval {
			$::querys->{$querys->{insert}}->execute(@fields);
		};
		if ($@) {
			print $@;
			print Dumper($::tables->{$table}->{fields});
			print Dumper($values);
			exit;
		}
		$record = __PACKAGE__->fetchrow_hashref($querys->{select_by_unique_fields}, @unique_fields);
		$action = 'inserted';
	}

	return {
		'record' => $record,
		'inserted' => ($action eq 'inserted' ? 1 : 0),
		'action' => $action,
		'fields_updated' => \@fields_updated
	};
}


sub get_single_value {
	my $self = shift;
	my $sql = shift || die 'query expected';

	my $sth = $::dbh->prepare($sql);
	$sth->execute();

	if (my $row = $sth->fetchrow_arrayref) {
		my $no_fields = scalar(@$row);
		if ($no_fields == 1) {
			return $row->[0];
		}
		else {
			die 'wrong number of fields returned 1 expected got '.$no_fields;
		}
	}
	else {
		die 'no rows returned';
	}
	return;
}

sub get_single_row_hashref {
	my $self = shift;
	my $sql = shift || die 'query expected';
}


sub get_single_row_arrayref {
	my $self = shift;
	my $sql = shift || die 'query expected';
}



=head1 fetchrow_hashref (really don't like this, shouldn't have same name methods as DBI)
	reduces execute & fetchrow_hashref into one operation for a given query handle defined in $::querys
=cut
sub fetchrow_hashref {
	my $self = shift;
	my $query_identifier = shift || confess 'no identifier';
	
	confess "query $query_identifier is not defined" if (!defined($::querys->{$query_identifier}));

	my $query_handle = $::querys->{$query_identifier};
	$query_handle->execute(@_);
	my $hashref = $query_handle->fetchrow_hashref();
	return $hashref;
}


sub current_date {
	my $self = shift;
	return POSIX::strftime("%d-%b-%y", gmtime);
}


sub set_schema {
	my $self = shift;
	my $schema_name = shift || die 'parameter missing';
	$schema_name = uc($schema_name);
	$::dbh->do("alter session set current_schema=\"$schema_name\"");
	#__PACKAGE__->get_schema_info($schema_name);
}

sub get_schema_info {
	my $self = shift;
	my $schema_name = shift;


	# should be local to module not global (bad practice)
	$::tables = {};
	$::querys = {};


	# get table names and column names
	my $sth = $::dbh->prepare("
		SELECT 
			dt.TABLE_NAME, 
			dtc.COLUMN_NAME,
			dtc.DATA_TYPE
		FROM dba_tables dt 
		JOIN dba_tab_columns dtc ON (dt.table_name = dtc.table_name) AND (dt.owner = dtc.owner) 
		WHERE dt.owner = ?
		ORDER BY dt.table_name, dtc.column_id");


	$sth->execute($schema_name);
	while (my $row = $sth->fetchrow_hashref) {
		push @{$::tables->{lc($row->{TABLE_NAME})}->{fields}}, $row->{COLUMN_NAME} ;
		push @{$::tables->{lc($row->{TABLE_NAME})}->{data_type}}, $row->{DATA_TYPE} ;
	}


	# get indexes, columns, and associated constraints
	$sth = $::dbh->prepare("
		SELECT 
			di.table_name, 
			dc.constraint_type, 
			di.index_name, 
			di.uniqueness,
			dic.column_name 
		FROM dba_indexes di 
		JOIN dba_ind_columns dic ON (di.index_name = dic.index_name) AND (di.owner = dic.index_owner)
		LEFT JOIN dba_constraints dc on (dc.index_name = di.index_name) and (dc.index_owner = di.owner)
		WHERE di.owner = ?
		AND uniqueness = 'UNIQUE'
		ORDER BY TABLE_NAME, INDEX_NAME, COLUMN_POSITION");

	$sth->execute($schema_name);

	while (my $row = $sth->fetchrow_hashref) {
		if (defined($row->{CONSTRAINT_TYPE}) && $row->{CONSTRAINT_TYPE} eq 'P') {
			$::tables->{lc($row->{TABLE_NAME})}->{primary_key} = $row->{COLUMN_NAME};
		}
		# could do with some work to separate individual unique constraints (probably quite rare)
		else {
			push @{$::tables->{lc($row->{TABLE_NAME})}->{unique_fields}}, $row->{COLUMN_NAME};
		}
	}

	my $sql = {};
	#print Dumper($::tables); exit;

	foreach my $table (keys(%$::tables)) {
		my $table_info = $::tables->{$table};

		my @unique_fields;

		if (defined($table_info->{unique_fields})) {
			@unique_fields = @{$table_info->{unique_fields}};
		}
		elsif(defined($table_info->{primary_key})) {
			@unique_fields = ( $table_info->{primary_key} );
		}
		else {
			foreach my $index (0..@{$table_info->{fields}}) {
				if ($table_info->{data_type}->[$index] ne 'CLOB') {
					@unique_fields = ( $table_info->{fields}->[$index] ); # use first field found which isn't a clob
					last;
				}
			}
		}

		$::tables->{$table}->{unique_fields} = \@unique_fields;


		if (@unique_fields) {
			$sql->{"select_".lc($table)."_by_unique_fields_new"} = 
			'SELECT * FROM '.uc($table)
			.' WHERE '.join(' AND ', map { "\"$_\" = ?" } @unique_fields);
			$sql->{"update_".lc($table)."_new"} = 'UPDATE '.uc($table)
			.' SET '.join(',', map { "$_ = ?" } @{$::tables->{$table}->{fields}})
			.' WHERE '.join(' AND ', map { "$_ = ?" } @unique_fields);
		}

		if ($table_info->{primary_key}) {
			$sql->{"select_".lc($table)."_max_new"} = 'SELECT MAX('.uc($table_info->{primary_key}).') '
			.uc($table_info->{primary_key}).' FROM '.uc($table);
		}

		$sql->{"insert_".lc($table)."_new"} = 'INSERT INTO '.uc($table).' ('.join(',', map { "\"$_\"" } @{$::tables->{$table}->{fields}}).')
		VALUES('.join(',', map { '?' } @{$::tables->{$table}->{fields}}).')';
	}


	foreach my $query_key (sort(keys(%$sql))) {
		eval {
			$::querys->{$query_key} = $::dbh->prepare($sql->{$query_key});
		};
		if ($@) {
			print $@;
			print "\n\nprepare failed (waiting 1 seconds...) $query_key = $sql->{$query_key}";
			sleep 1;
		}
		#print "$query_key\n";
	}
	#exit;
}


1;

